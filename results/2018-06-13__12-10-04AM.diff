diff --git a/agents/bootstrap_rl/train/train.py b/agents/bootstrap_rl/train/train.py
index 4d4d7b5..a70a9be 100644
--- a/agents/bootstrap_rl/train/train.py
+++ b/agents/bootstrap_rl/train/train.py
@@ -101,7 +101,7 @@ def run(env_id, bootstrap_net_path,
                 mlp_width = 5
                 minibatch_steps = 16
             else:
-                minibatch_steps = 80
+                minibatch_steps = 160
                 mlp_width = 64
             train(bootstrap_gym_env, seed=c.RNG_SEED, sess=sess_2, is_discrete=is_discrete,
                   minibatch_steps=minibatch_steps, mlp_width=mlp_width)
diff --git a/config.py b/config.py
index 2a67eb5..8b4cbe9 100644
--- a/config.py
+++ b/config.py
@@ -125,3 +125,11 @@ DEFAULT_FPS = 8
 
 # Experimental stuff - not worth passing as main.py args yet, but better for reproducing to put here than in os.environ
 SIMPLE_PPO = False
+
+try:
+    import tensorflow
+except ImportError:
+    TENSORFLOW_AVAILABLE = False
+else:
+    TENSORFLOW_AVAILABLE = True
+
diff --git a/gym_deepdrive/envs/deepdrive_gym_env.py b/gym_deepdrive/envs/deepdrive_gym_env.py
index 54ca5bd..2ad6e9c 100644
--- a/gym_deepdrive/envs/deepdrive_gym_env.py
+++ b/gym_deepdrive/envs/deepdrive_gym_env.py
@@ -158,7 +158,7 @@ class DrivingStyle(Enum):
     __order__ = 'CRUISING NORMAL LATE EMERGENCY CHASE'
     # TODO: Possibly assign function rather than just weights
     CRUISING   = RewardWeighting(speed=0.5, progress=0.0, gforce=2.00, lane_deviation=1.50, total_time=0.0)
-    NORMAL     = RewardWeighting(speed=1.0, progress=0.0, gforce=1.00, lane_deviation=1.00, total_time=0.0)
+    NORMAL     = RewardWeighting(speed=1.0, progress=0.0, gforce=0.00, lane_deviation=0.00, total_time=0.0)
     LATE       = RewardWeighting(speed=2.0, progress=0.0, gforce=0.50, lane_deviation=0.50, total_time=0.0)
     EMERGENCY  = RewardWeighting(speed=2.0, progress=0.0, gforce=0.75, lane_deviation=0.75, total_time=0.0)
     CHASE      = RewardWeighting(speed=2.0, progress=0.0, gforce=0.00, lane_deviation=0.00, total_time=0.0)
@@ -269,6 +269,13 @@ class DeepDriveEnv(gym.Env):
             self.git_diff = None
             log.warning('Could not get git diff for associating benchmark results with code state')
 
+        if c.TENSORFLOW_AVAILABLE:
+            import tensorflow as tf
+            self.tensorboard_writer = tf.summary.FileWriter(
+                os.path.join(c.TENSORFLOW_OUT_DIR, 'env', c.DATE_STR))
+        else:
+            self.tensorboard_writer = None
+
     def open_sim(self):
         self._kill_competing_procs()
         if c.REUSE_OPEN_SIM:
@@ -380,6 +387,7 @@ class DeepDriveEnv(gym.Env):
         self.sess = session
 
     def step(self, action):
+        info = {}
         if self.is_discrete:
             steer, throttle, brake = self.discrete_actions.get_components(action)
             dd_action = Action(steering=steer, throttle=throttle, brake=brake)
@@ -397,8 +405,8 @@ class DeepDriveEnv(gym.Env):
         done = False
 
         start_reward_stuff = time.time()
-        reward = self.get_reward(obz, now)
-        done = self.compute_lap_statistics(done, obz)
+        reward, done = self.get_reward(obz, now)
+        done = self.compute_lap_statistics(done, obz) or done
         self.prev_step_time = now
 
         if self.dashboard_pub is not None:
@@ -406,13 +414,12 @@ class DeepDriveEnv(gym.Env):
             self.dashboard_pub.put(OrderedDict({'display_stats': list(self.display_stats.items()), 'should_stop': False}))
             log.debug('dashboard put took %fs', time.time() - start_dashboard_put)
 
-        if self.is_stuck(obz) or self.driving_wrong_way():  # TODO: derive this from collision, time elapsed, and distance as well
-            done = True
-            reward -= 70  # reward is in scale of meters
-        info = {}
         self.step_num += 1
         log.debug('reward stuff took %fs', time.time() - start_reward_stuff)
 
+        if done:
+            info = self.report_score(info)
+
         self.regulate_fps()
 
         if self.should_render:
@@ -420,6 +427,32 @@ class DeepDriveEnv(gym.Env):
 
         return obz, reward, done, info
 
+    def report_score(self, info):
+        self.prev_lap_score = self.score.total
+        info['episode'] = episode_info = {}
+        episode_info['reward'] = self.score.total
+        episode_info['length'] = self.step_num
+        if self.should_benchmark:
+            self.log_benchmark_trial()
+        else:
+            log.info('lap %d complete with score of %f', self.total_laps, self.score.total)
+        if self.tensorboard_writer is not None:
+            import tensorflow as tf
+            summary = tf.Summary()
+            summary.value.add(tag="score/total", simple_value=self.score.total)
+            summary.value.add(tag="score/episode_length", simple_value=self.step_num)
+            summary.value.add(tag="score/episode_time", simple_value=self.score.episode_time)
+            summary.value.add(tag="score/speed_reward", simple_value=self.score.speed_reward)
+
+            summary.value.add(tag="score/lane_deviation_penalty", simple_value=self.score.lane_deviation_penalty)
+            summary.value.add(tag="score/gforce_penalty", simple_value=self.score.gforce_penalty)
+            summary.value.add(tag="score/got_stuck", simple_value=self.score.got_stuck)
+            summary.value.add(tag="score/time_penalty", simple_value=self.score.time_penalty)
+
+            self.tensorboard_writer.add_summary(summary)
+            self.tensorboard_writer.flush()
+        return info
+
     def regulate_fps(self):
         now = time.time()
         if self.previous_action_time:
@@ -441,14 +474,6 @@ class DeepDriveEnv(gym.Env):
         lap_number = obz.get('lap_number')
         if lap_number is not None and self.lap_number is not None and self.lap_number < lap_number:
             self.total_laps += 1
-            self.prev_lap_score = self.score.total
-            if self.should_benchmark:
-                self.log_benchmark_trial()
-                if len(self.trial_scores) >= 50:
-                    self.done_benchmarking = True
-            else:
-                log.info('lap %d complete with score of %f', self.total_laps, self.score.total)
-
             done = True  # One lap per episode
             self.log_up_time()
         self.lap_number = lap_number
@@ -459,6 +484,7 @@ class DeepDriveEnv(gym.Env):
     def get_reward(self, obz, now):
         start_get_reward = time.time()
         reward = 0
+        done = False
         if obz:
             if self.is_sync:
                 step_time = self.sync_step_time
@@ -479,10 +505,10 @@ class DeepDriveEnv(gym.Env):
                 time_penalty = self.get_time_penalty(obz, step_time)
                 reward = self.combine_rewards(progress_reward, gforce_penalty, lane_deviation_penalty,
                                               time_penalty, speed)
-                if self.score.episode_time < 2:
-                    # Speed reward is too big at reset due to small offset between origin and spawn, so clip it to
-                    # avoid incenting resets
-                    reward = min(max(reward, -1), 1)
+
+            if self.is_stuck(obz) or self.driving_wrong_way():  # TODO: derive this from collision, time elapsed, and distance as well
+                done = True
+                reward -= 1
 
             self.score.total += reward
             self.display_stats['time']['value'] = self.score.episode_time
@@ -495,7 +521,7 @@ class DeepDriveEnv(gym.Env):
 
         log.debug('get reward took %fs', time.time() - start_get_reward)
 
-        return reward
+        return reward, done
 
     def log_up_time(self):
         log.info('up for %r' % arrow.get(time.time()).humanize(other=arrow.get(self.start_time), only_distance=True))
@@ -538,11 +564,17 @@ class DeepDriveEnv(gym.Env):
             if self.distance_along_route:
                 self.previous_distance_along_route = self.distance_along_route
             self.distance_along_route = dist
-            progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_reward(progress, time_passed)
+            progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_and_speed_reward(progress, time_passed)
 
         progress_reward *= self.driving_style.value.progress_weight
         speed_reward *= self.driving_style.value.speed_weight
 
+        if self.score.episode_time < 2:
+            # Speed reward is too big at reset due to small offset between origin and spawn, so clip it to
+            # avoid incenting resets
+            speed_reward = min(max(speed_reward, -1), 1)
+            progress_reward = min(max(progress_reward, -1), 1)
+
         self.score.progress_reward += progress_reward
         self.score.speed_reward += speed_reward
 
@@ -587,7 +619,6 @@ class DeepDriveEnv(gym.Env):
                 self.set_forward_progress()
                 if self.should_benchmark:
                     self.score.got_stuck = True
-                    self.log_benchmark_trial()
                 ret = True
         else:
             self.set_forward_progress()
@@ -1033,7 +1064,7 @@ class DeepDriveRewardCalculator(object):
         return gforce_penalty
 
     @staticmethod
-    def get_progress_reward(progress, time_passed):
+    def get_progress_and_speed_reward(progress, time_passed):
         if not time_passed:
             progress = speed_reward = 0
         else:
diff --git a/tests/test_sanity.py b/tests/test_sanity.py
index c8ace89..798eabe 100644
--- a/tests/test_sanity.py
+++ b/tests/test_sanity.py
@@ -66,21 +66,21 @@ def test_gforce_penalty():
 
 
 def test_progress_reward():
-    progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_reward(progress=100, time_passed=0.1)
+    progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_and_speed_reward(progress=100, time_passed=0.1)
     assert progress_reward == pytest.approx(1.) and speed_reward == pytest.approx(1.5)
-    progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_reward(progress=100, time_passed=1)
+    progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_and_speed_reward(progress=100, time_passed=1)
     assert progress_reward == pytest.approx(1.) and speed_reward == pytest.approx(0.15)
-    progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_reward(progress=3, time_passed=0.1)
+    progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_and_speed_reward(progress=3, time_passed=0.1)
     assert progress_reward == pytest.approx(0.03) and speed_reward == pytest.approx(0.00135)
-    progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_reward(progress=3, time_passed=1e-8)
+    progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_and_speed_reward(progress=3, time_passed=1e-8)
     assert progress_reward == pytest.approx(0.03, abs=1e-6) and speed_reward == pytest.approx(100.0)  # Should clip
-    progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_reward(progress=0, time_passed=0.1)
+    progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_and_speed_reward(progress=0, time_passed=0.1)
     assert progress_reward == pytest.approx(0.) and speed_reward == pytest.approx(0.)
-    progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_reward(progress=-10, time_passed=0.1)
+    progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_and_speed_reward(progress=-10, time_passed=0.1)
     assert progress_reward == pytest.approx(-0.1) and speed_reward == pytest.approx(-0.015)
-    progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_reward(progress=1e8, time_passed=0.1)
+    progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_and_speed_reward(progress=1e8, time_passed=0.1)
     assert progress_reward == pytest.approx(100.) and speed_reward == pytest.approx(100.)  # Should clip
-    progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_reward(progress=-1e8, time_passed=0.1)
+    progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_and_speed_reward(progress=-1e8, time_passed=0.1)
     assert progress_reward == pytest.approx(0.) and speed_reward == pytest.approx(0.)  # lap complete, zero out
 
     # Test invariance of sampling frequency
@@ -95,8 +95,8 @@ def episode_progress_reward(hz, total_secs):
     time_passed = 1 / hz
     progress = 1000 / hz
     for i in range(int(total_secs * hz)):
-        progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_reward(progress=progress,
-                                                                                     time_passed=time_passed)
+        progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_and_speed_reward(progress=progress,
+                                                                                                time_passed=time_passed)
         total_progress += progress_reward
         total_speed_reward += speed_reward
     return total_progress, total_speed_reward
diff --git a/vendor/openai/baselines/a2c/utils.py b/vendor/openai/baselines/a2c/utils.py
index 146c49a..107d15c 100644
--- a/vendor/openai/baselines/a2c/utils.py
+++ b/vendor/openai/baselines/a2c/utils.py
@@ -62,9 +62,9 @@ def fc(x, scope, nh, *, init_scale=1.0, init_bias=0.0):
     with tf.variable_scope(scope):
         nin = x.get_shape()[1].value
         w = tf.get_variable("w", [nin, nh], initializer=ortho_init(init_scale))
-        w = tf.Print(w, ['w ', scope, w], summarize=100)
+        # w = tf.Print(w, ['w ', scope, w], summarize=100)
         b = tf.get_variable("b", [nh], initializer=tf.constant_initializer(init_bias))
-        b = tf.Print(b, ['b ', scope, w], summarize=100)
+        # b = tf.Print(b, ['b ', scope, w], summarize=100)
 
         return tf.matmul(x, w)+b
 
diff --git a/vendor/openai/baselines/common/continuous_action_wrapper.py b/vendor/openai/baselines/common/continuous_action_wrapper.py
index 0f2c479..de13e6e 100644
--- a/vendor/openai/baselines/common/continuous_action_wrapper.py
+++ b/vendor/openai/baselines/common/continuous_action_wrapper.py
@@ -1,4 +1,5 @@
 import gym
+import gym.spaces
 
 
 class CombineBoxSpaceWrapper(gym.Wrapper):
diff --git a/vendor/openai/baselines/common/vec_env/dummy_vec_env.py b/vendor/openai/baselines/common/vec_env/dummy_vec_env.py
index 0c5a6c7..8ad512d 100644
--- a/vendor/openai/baselines/common/vec_env/dummy_vec_env.py
+++ b/vendor/openai/baselines/common/vec_env/dummy_vec_env.py
@@ -1,5 +1,6 @@
 import numpy as np
 import gym
+import gym.spaces
 from . import VecEnv
 
 class DummyVecEnv(VecEnv):
diff --git a/vendor/openai/baselines/ppo2/policies.py b/vendor/openai/baselines/ppo2/policies.py
index 7d96251..7cb4b69 100644
--- a/vendor/openai/baselines/ppo2/policies.py
+++ b/vendor/openai/baselines/ppo2/policies.py
@@ -224,25 +224,24 @@ class MlpPolicy(object):
         a0 = self.pd.sample()
 
         neglogp0 = self.pd.neglogp(a0)
+        action_probs0 = tf.exp(-neglogp0)
+
         self.initial_state = None
 
         def step(ob, *_args, **_kwargs):
-            if c.SIMPLE_PPO:
-                a, v, neglogp, p_w0 = sess.run([a0, vf, neglogp0, self.p_h1], {X:ob})
-                print('pw0', p_w0)
-            else:
-                a, v, neglogp = sess.run([a0, vf, neglogp0], {X: ob})
+            a, v, neglogp, action_probs = sess.run([a0, vf, neglogp0, action_probs0], {X: ob})
 
             # For deepdrive we expect outputs to be between -1 and 1 - let's just max out actions for now
             # a = np.tanh(a)
 
-            return a, v, self.initial_state, neglogp
+            return a, v, self.initial_state, neglogp, action_probs
 
         def value(ob, *_args, **_kwargs):
             return sess.run(vf, {X: ob})
 
         self.X = X
         self.pi = pi
+        self.action_probs0 = action_probs0
         self.vf = vf
         self.step = step
         self.value = value
diff --git a/vendor/openai/baselines/ppo2/ppo2.py b/vendor/openai/baselines/ppo2/ppo2.py
index 59beb8c..27868a5 100644
--- a/vendor/openai/baselines/ppo2/ppo2.py
+++ b/vendor/openai/baselines/ppo2/ppo2.py
@@ -103,6 +103,31 @@ class Model(object):
         tf.global_variables_initializer().run(session=sess) #pylint: disable=E1101
 
 
+def mis(action_probs, rewards):
+    """ Mistake importance scaling
+    It seems that taking the log probability in Policy Gradient reverses the amount of learning you would want for
+    negative rewards. i.e. We learn much more from unlikely bad actions, than we do likely ones. Whereas this is what
+    we want for positive rewards - to learn more from unlikely good actions, we would want the opposite for negative
+    rewards - learn more from likely bad actions because our goal is for bad actions and states to be unlikely.
+    I've tested these ideas a bit in baselines and the results seem to be good.
+    Although I'm sort of duct-taping on the idea by scaling negative rewards inversely to their odds to reverse
+    the effect of taking the log. I also notice that DQN, which does not scale the gradient by log likelihood,
+    does better than PG methods on Atari games with mostly negative rewards, i.e. DoubleDunk, ice hockey, and surround,
+    with skiing being an exception to this rule - but the score for skiing is weird."""
+    mis_rewards = []
+    for i, reward in enumerate(rewards):
+        if 'SCALE_ALL_REWARDS' in os.environ:
+            mis_rewards.append(reward * 1.8)  # Works (in pong), but not as well as scaling by odds
+        else:
+            if reward < 0:
+                scale = 1 + action_probs[i] / (1 - action_probs[i])
+                scale = min(scale, 3)
+                mis_rewards.append(reward * scale)
+            else:
+                mis_rewards.append(reward)
+    return mis_rewards
+
+
 class Runner(object):
 
     def __init__(self, *, env, model, nsteps, gamma, lam):
@@ -122,7 +147,8 @@ class Runner(object):
         mb_states = self.states
         epinfos = []
         for _ in range(self.nsteps):
-            actions, values, self.states, neglogpacs = self.model.step(self.obs, self.states, self.dones)
+            actions, values, self.states, neglogpacs, action_probs = self.model.step(self.obs, self.states, self.dones)
+
             mb_obs.append(self.obs.copy())
             mb_actions.append(actions)
             mb_values.append(values)
@@ -131,9 +157,12 @@ class Runner(object):
 
             self.obs[:], rewards, self.dones, infos = self.env.step(actions)
 
+            rewards = mis(action_probs, rewards)
+
             for info in infos:
                 maybe_episode_info = info.get('episode') if info else None
                 if maybe_episode_info: epinfos.append(maybe_episode_info)
+
             mb_rewards.append(rewards)
         #batch of steps to batch of rollouts
         mb_obs = np.asarray(mb_obs, dtype=self.obs.dtype)
@@ -274,8 +303,8 @@ def learn(*, policy, env, nsteps, total_timesteps, ent_coef, lr,
             logger.logkv("total_timesteps", update * nbatch)
             logger.logkv("fps", fps)
             logger.logkv("explained_variance", float(ev))
-            logger.logkv('eprewmean', safemean([epinfo['r'] for epinfo in epinfobuf]))
-            logger.logkv('eplenmean', safemean([epinfo['l'] for epinfo in epinfobuf]))
+            logger.logkv('eprewmean', safemean([epinfo['reward'] for epinfo in epinfobuf]))
+            logger.logkv('eplenmean', safemean([epinfo['length'] for epinfo in epinfobuf]))
             logger.logkv('time_elapsed', tnow - tfirststart)
             for (lossval, lossname) in zip(lossvals, model.loss_names):
                 logger.logkv(lossname, lossval)
diff --git a/vendor/openai/baselines/ppo2/run_deepdrive.py b/vendor/openai/baselines/ppo2/run_deepdrive.py
index 589b1a7..7173542 100644
--- a/vendor/openai/baselines/ppo2/run_deepdrive.py
+++ b/vendor/openai/baselines/ppo2/run_deepdrive.py
@@ -56,7 +56,7 @@ def train(env, seed, sess=None, is_discrete=True, minibatch_steps=None, mlp_widt
                noptepochs=3,
                log_interval=1,
                ent_coef=0.0,
-               lr=lambda f: f * 2.5e-4,
+               lr=lambda f: f * 2.5e-2,
                cliprange=lambda f: f * 0.1,
                total_timesteps=int(1e5),
                mlp_width=mlp_width)