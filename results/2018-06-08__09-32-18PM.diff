diff --git a/agents/bootstrap_rl/train/train.py b/agents/bootstrap_rl/train/train.py
index 7c28fb7..b0589f6 100644
--- a/agents/bootstrap_rl/train/train.py
+++ b/agents/bootstrap_rl/train/train.py
@@ -77,7 +77,7 @@ def run(env_id, bootstrap_net_path,
             # Wrap step so we get the pretrained layer activations rather than pixels for our observation
             bootstrap_gym_env = BootstrapRLGymEnv(dagger_gym_env, dagger_agent)
 
-            train(bootstrap_gym_env, num_timesteps=int(10e6), seed=c.RNG_SEED, sess=sess_2, is_discrete=is_discrete)
+            train(bootstrap_gym_env, num_timesteps=int(18e3), seed=c.RNG_SEED, sess=sess_2, is_discrete=is_discrete)
     #
     # action = deepdrive.action()
     # while not done:
diff --git a/vendor/openai/baselines/ppo2/run_deepdrive.py b/vendor/openai/baselines/ppo2/run_deepdrive.py
index 69ee9f7..bba190b 100644
--- a/vendor/openai/baselines/ppo2/run_deepdrive.py
+++ b/vendor/openai/baselines/ppo2/run_deepdrive.py
@@ -47,6 +47,7 @@ def train(env, num_timesteps, seed, sess=None, is_discrete=True):
                nminibatches=1,  # Sweet spot is between 16 and 64 for continuous mountain car @55fps
                lam=0.95,
                gamma=0.99,
+               save_interval=1,
                noptepochs=3,
                log_interval=1,
                ent_coef=0.0,