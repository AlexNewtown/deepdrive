diff --git a/agents/bootstrap_rl/train/train.py b/agents/bootstrap_rl/train/train.py
index 2e231a8..56c476e 100644
--- a/agents/bootstrap_rl/train/train.py
+++ b/agents/bootstrap_rl/train/train.py
@@ -25,7 +25,7 @@ class BootstrapRLGymEnv(gym.Wrapper):
         self.observation_space = spaces.Box(low=np.finfo(np.float32).min,
                                             high=np.finfo(np.float32).max,
                                             # shape=(c.ALEXNET_FC7 + c.NUM_TARGETS,),
-                                            shape=(dagger_agent.net.num_last_hidden,),
+                                            shape=(dagger_agent.net.num_last_hidden + dagger_agent.net.num_targets,),
                                             dtype=np.float32)
 
     def step(self, action):
@@ -34,7 +34,7 @@ class BootstrapRLGymEnv(gym.Wrapper):
         if net_out is None:
             obz = None
         else:
-            obz = np.concatenate((net_out[0][0], net_out[1][0]))
+            obz = np.concatenate((np.squeeze(net_out[0]), np.squeeze(net_out[1])))
         return obz, reward, done, info
 
     def reset(self):
diff --git a/agents/dagger/agent.py b/agents/dagger/agent.py
index 8d29c1d..b96d962 100644
--- a/agents/dagger/agent.py
+++ b/agents/dagger/agent.py
@@ -268,7 +268,7 @@ class Agent(object):
         else:
             out_var = self.net.out
         if self.output_last_hidden:
-            out_var = [out_var, self.net.fc7]
+            out_var = [out_var, self.net.last_hidden]
 
         image = image.reshape(1, *self.net.input_image_shape)
         net_out = self.sess.run(out_var, feed_dict={
diff --git a/camera_config.py b/camera_config.py
index b210a0e..015ddd1 100644
--- a/camera_config.py
+++ b/camera_config.py
@@ -3,7 +3,9 @@ import config
 
 log = logs.get_log(__name__)
 
-# First dimension is rotated through at the end of the episode. Second dimension is for separate cameras on the car.
+# Rigs are two dimensional arrays where...
+# cameras in the first dimension are rotated through at the end of the episode during recording and...
+# cameras in the second dimension create multiple simultaneously rendering views from the vehicle.
 rigs = {
     'baseline_rigs': [
         [config.DEFAULT_CAM],
diff --git a/gym_deepdrive/envs/deepdrive_gym_env.py b/gym_deepdrive/envs/deepdrive_gym_env.py
index 9274ae9..8425c6a 100644
--- a/gym_deepdrive/envs/deepdrive_gym_env.py
+++ b/gym_deepdrive/envs/deepdrive_gym_env.py
@@ -77,8 +77,13 @@ class Action(object):
         has_control = True
         if len(action) > 4:
             has_control = action[4][0]
+        handbrake = action[3][0]
+        if handbrake < 0:
+            handbrake = 0
+        else:
+            handbrake = 1
         ret = cls(steering=action[0][0], throttle=action[1][0],
-                  brake=action[2][0], handbrake=action[3][0], has_control=has_control)
+                  brake=action[2][0], handbrake=handbrake, has_control=has_control)
         return ret
 
 
diff --git a/vendor/openai/baselines/common/continuous_action_wrapper.py b/vendor/openai/baselines/common/continuous_action_wrapper.py
index 4a9d722..a8a439a 100644
--- a/vendor/openai/baselines/common/continuous_action_wrapper.py
+++ b/vendor/openai/baselines/common/continuous_action_wrapper.py
@@ -15,7 +15,10 @@ class CombineBoxSpaceWrapper(gym.Wrapper):
                     raise NotImplementedError('Multi-dimensional box spaces not yet supported - need to flatten / separate')
                 else:
                     total_dims += 1
-                denormalizer = lambda x: ((x + 1) * (space.high - space.low) / 2 + space.low)
+
+                def denormalizer(x):
+                    ret = ((x + 1) * (space.high[0] - space.low[0]) / 2 + space.low[0])
+                    return ret
                 self.denormalizers.append(denormalizer)
             self.action_space = gym.spaces.Box(-1, 1, shape=(total_dims,))
 
@@ -23,9 +26,9 @@ class CombineBoxSpaceWrapper(gym.Wrapper):
         # Denormalize action according to mapping
         denorm_action = []
         for i, denorm in enumerate(self.denormalizers):
-            denorm_action.append(denorm(action[i]))
+            denorm_action.append([denorm(action[i])])
         obz, reward, done, info = self.env.step(denorm_action)
         return obz, reward, done, info
 
     def reset(self):
-        return self.env.reset()
\ No newline at end of file
+        return self.env.reset()
diff --git a/vendor/openai/baselines/common/vec_env/dummy_vec_env.py b/vendor/openai/baselines/common/vec_env/dummy_vec_env.py
index 3f31e18..2bf2177 100644
--- a/vendor/openai/baselines/common/vec_env/dummy_vec_env.py
+++ b/vendor/openai/baselines/common/vec_env/dummy_vec_env.py
@@ -37,8 +37,10 @@ class DummyVecEnv(VecEnv):
     def reset(self):
         for i in range(self.num_envs):
             obs_tuple = self.envs[i].reset()
+            if obs_tuple is None:
+                obs_tuple = 0.
             if isinstance(obs_tuple, (tuple, list)):
-                for t,x in enumerate(obs_tuple):
+                for t, x in enumerate(obs_tuple):
                     self.buf_obs[t][i] = x
             else:
                 self.buf_obs[0][i] = obs_tuple
diff --git a/vendor/openai/baselines/ppo2/policies.py b/vendor/openai/baselines/ppo2/policies.py
index 7c5284c..ee59f68 100644
--- a/vendor/openai/baselines/ppo2/policies.py
+++ b/vendor/openai/baselines/ppo2/policies.py
@@ -217,7 +217,7 @@ class MlpPolicy(object):
             return a, v, self.initial_state, neglogp
 
         def value(ob, *_args, **_kwargs):
-            return sess.run(vf, {X:ob})
+            return sess.run(vf, {X: ob})
 
         self.X = X
         self.pi = pi
diff --git a/vendor/openai/baselines/ppo2/ppo2.py b/vendor/openai/baselines/ppo2/ppo2.py
index 5926176..341498f 100644
--- a/vendor/openai/baselines/ppo2/ppo2.py
+++ b/vendor/openai/baselines/ppo2/ppo2.py
@@ -118,7 +118,6 @@ class Runner(object):
             mb_neglogpacs.append(neglogpacs)
             mb_dones.append(self.dones)
 
-
             self.obs[:], rewards, self.dones, infos = self.env.step(actions)
 
             for info in infos:
@@ -259,6 +258,7 @@ def learn(*, policy, env, nsteps, total_timesteps, ent_coef, lr,
             for (lossval, lossname) in zip(lossvals, model.loss_names):
                 logger.logkv(lossname, lossval)
             logger.dumpkvs()
+            # input('continue?')
         if save_interval and (update % save_interval == 0 or update == 1) and logger.get_dir():
             checkdir = osp.join(logger.get_dir(), 'checkpoints')
             os.makedirs(checkdir, exist_ok=True)
diff --git a/vendor/openai/baselines/ppo2/run_deepdrive.py b/vendor/openai/baselines/ppo2/run_deepdrive.py
index 5feb9bb..9022767 100644
--- a/vendor/openai/baselines/ppo2/run_deepdrive.py
+++ b/vendor/openai/baselines/ppo2/run_deepdrive.py
@@ -24,7 +24,7 @@ def train(env, num_timesteps, seed, sess=None, is_discrete=True):
         tf.Session(config=config).__enter__()
 
     env = DummyVecEnv(envs=[env])
-    env = VecNormalize(env)
+    env = VecNormalize(env, ob=False)
 
     set_global_seeds(seed)
     if 'LSTM_FLAT' in os.environ:
@@ -47,7 +47,7 @@ def train(env, num_timesteps, seed, sess=None, is_discrete=True):
                noptepochs=3,
                log_interval=1,
                ent_coef=0.0,
-               lr=lambda f: f * 2.5e-4,
+               lr=lambda f: f * 2.5e-8,
                cliprange=lambda f: f * 0.1,
                total_timesteps=num_timesteps)