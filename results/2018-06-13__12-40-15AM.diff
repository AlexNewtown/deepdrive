diff --git a/gym_deepdrive/envs/deepdrive_gym_env.py b/gym_deepdrive/envs/deepdrive_gym_env.py
index 2ad6e9c..90e35c3 100644
--- a/gym_deepdrive/envs/deepdrive_gym_env.py
+++ b/gym_deepdrive/envs/deepdrive_gym_env.py
@@ -402,7 +402,6 @@ class DeepDriveEnv(gym.Env):
         if obz and 'is_game_driving' in obz:
             self.has_control = not obz['is_game_driving']
         now = time.time()
-        done = False
 
         start_reward_stuff = time.time()
         reward, done = self.get_reward(obz, now)
diff --git a/vendor/openai/baselines/ppo2/policies.py b/vendor/openai/baselines/ppo2/policies.py
index 7cb4b69..b8cfdef 100644
--- a/vendor/openai/baselines/ppo2/policies.py
+++ b/vendor/openai/baselines/ppo2/policies.py
@@ -232,7 +232,7 @@ class MlpPolicy(object):
             a, v, neglogp, action_probs = sess.run([a0, vf, neglogp0, action_probs0], {X: ob})
 
             # For deepdrive we expect outputs to be between -1 and 1 - let's just max out actions for now
-            # a = np.tanh(a)
+            a = np.tanh(a)
 
             return a, v, self.initial_state, neglogp, action_probs