diff --git a/agents/bootstrap_rl/train/train.py b/agents/bootstrap_rl/train/train.py
index b0589f6..74cc43e 100644
--- a/agents/bootstrap_rl/train/train.py
+++ b/agents/bootstrap_rl/train/train.py
@@ -5,8 +5,10 @@ from gym import spaces
 
 import deepdrive
 import config as c
+from agents.common import get_throttle
 from agents.dagger.agent import Agent
 from agents.dagger.net import MOBILENET_V2_NAME
+from gym_deepdrive.envs.deepdrive_gym_env import Action
 from vendor.openai.baselines.ppo2.run_deepdrive import train
 
 
@@ -14,6 +16,7 @@ class BootstrapRLGymEnv(gym.Wrapper):
     def __init__(self, env, dagger_agent):
         super(BootstrapRLGymEnv, self).__init__(env)
         self.dagger_agent = dagger_agent
+        self.previous_obz = None
 
         # One thing we need to do here is to make each action a bi-modal guassian to avoid averaging 50/50 decisions
         # i.e. half the time we veer left, half the time veer right - but on average this is go straight and can run us
@@ -29,8 +32,16 @@ class BootstrapRLGymEnv(gym.Wrapper):
                                             dtype=np.float32)
 
     def step(self, action):
+        # if self.previous_obz is not None:
+        #     # Simplifying by only controlling steering. Otherwise, we need to shape rewards so that initial acceleration
+        #     # is not disincentivized by gforce penalty.
+        #     action[Action.THROTTLE_INDEX] = get_throttle(actual_speed=self.previous_obz['speed'],
+        #                                                  target_speed=(8 * 100))
+
         obz, reward, done, info = self.env.step(action)
+        self.previous_obz = obz
         action, net_out = self.dagger_agent.act(obz, reward, done)
+        info['dagger_agent_action'] = action
         if net_out is None:
             obz = None
         else:
@@ -77,7 +88,7 @@ def run(env_id, bootstrap_net_path,
             # Wrap step so we get the pretrained layer activations rather than pixels for our observation
             bootstrap_gym_env = BootstrapRLGymEnv(dagger_gym_env, dagger_agent)
 
-            train(bootstrap_gym_env, num_timesteps=int(18e3), seed=c.RNG_SEED, sess=sess_2, is_discrete=is_discrete)
+            train(bootstrap_gym_env, num_timesteps=int(1e6), seed=c.RNG_SEED, sess=sess_2, is_discrete=is_discrete)
     #
     # action = deepdrive.action()
     # while not done:
diff --git a/agents/common.py b/agents/common.py
index e69de29..9a578ea 100644
--- a/agents/common.py
+++ b/agents/common.py
@@ -0,0 +1,23 @@
+from __future__ import (absolute_import, division,
+                        print_function, unicode_literals)
+
+import glob
+import os
+
+from future.builtins import (ascii, bytes, chr, dict, filter, hex, input,
+                             int, map, next, oct, open, pow, range, round,
+                             str, super, zip)
+
+import sys
+
+import logs
+import config as c
+
+
+log = logs.get_log(__name__)
+
+
+def get_throttle(actual_speed, target_speed):
+    desired_throttle = abs(target_speed / max(actual_speed, 1e-3))
+    desired_throttle = min(max(desired_throttle, 0.), 1.)
+    return desired_throttle
diff --git a/agents/dagger/agent.py b/agents/dagger/agent.py
index 4731d45..2d2505b 100644
--- a/agents/dagger/agent.py
+++ b/agents/dagger/agent.py
@@ -10,6 +10,7 @@ import numpy as np
 
 import config as c
 import deepdrive
+from agents.common import get_throttle
 from agents.dagger import net
 from agents.dagger.train.train import resize_images
 from gym_deepdrive.envs.deepdrive_gym_env import Action, Urgency
@@ -146,8 +147,7 @@ class Agent(object):
             # desired_throttle = abs(target_speed / max(actual_speed, 1e-3))
             # desired_throttle = min(max(desired_throttle, 0.), 1.)
             target_speed = 8 * 100
-            desired_throttle = abs(target_speed / max(actual_speed, 1e-3))
-            desired_throttle = min(max(desired_throttle, 0.), 1.)
+            desired_throttle = get_throttle(actual_speed, target_speed)
 
             # if self.previous_net_out:
             #     desired_throttle = 0.2 * self.previous_action.throttle + 0.7 * desired_throttle
@@ -168,8 +168,7 @@ class Agent(object):
 
             # Network overfit on speed, plus it's nice to be able to change it,
             # so we just ignore output speed of net
-            desired_throttle = abs(target_speed / max(actual_speed, 1e-3))
-            desired_throttle = min(max(desired_throttle, 0.), 1.)
+            desired_throttle = get_throttle(actual_speed, target_speed)
         log.debug('actual_speed %r' % actual_speed)
 
         # log.info('desired_steering %f', desired_steering)
diff --git a/gym_deepdrive/envs/deepdrive_gym_env.py b/gym_deepdrive/envs/deepdrive_gym_env.py
index 27f2b58..d736557 100644
--- a/gym_deepdrive/envs/deepdrive_gym_env.py
+++ b/gym_deepdrive/envs/deepdrive_gym_env.py
@@ -62,6 +62,12 @@ class Score(object):
 
 
 class Action(object):
+    STEERING_INDEX = 0
+    THROTTLE_INDEX = 1
+    BRAKE_INDEX = 2
+    HANDBRAKE_INDEX = 3
+    HAS_CONTROL_INDEX = 4
+
     def __init__(self, steering=0, throttle=0, brake=0, handbrake=0, has_control=True):
         self.steering = steering
         self.throttle = throttle
@@ -81,14 +87,16 @@ class Action(object):
             if isinstance(action[4], list):
                 has_control = action[4][0]
             else:
-                has_control = action[4]
-        handbrake = action[3][0]
+                has_control = action[cls.HANDBRAKE_INDEX]
+        handbrake = action[cls.HANDBRAKE_INDEX][0]
         if handbrake <= 0 or math.isnan(handbrake):
             handbrake = 0
         else:
             handbrake = 1
-        ret = cls(steering=action[0][0], throttle=action[1][0],
-                  brake=action[2][0], handbrake=handbrake, has_control=has_control)
+        ret = cls(steering=action[cls.STEERING_INDEX][0],
+                  throttle=action[cls.THROTTLE_INDEX][0],
+                  brake=action[cls.BRAKE_INDEX][0],
+                  handbrake=handbrake, has_control=has_control)
         return ret
 
 
diff --git a/vendor/openai/baselines/ppo2/ppo2.py b/vendor/openai/baselines/ppo2/ppo2.py
index d0d8122..bdedff7 100644
--- a/vendor/openai/baselines/ppo2/ppo2.py
+++ b/vendor/openai/baselines/ppo2/ppo2.py
@@ -8,6 +8,9 @@ import joblib
 import numpy as np
 # noinspection PyPackageRequirements
 import tensorflow as tf
+
+from agents.common import get_throttle
+from gym_deepdrive.envs.deepdrive_gym_env import Action
 from vendor.openai.baselines import logger
 
 from vendor.openai.baselines.common.math_util import explained_variance
@@ -158,6 +161,14 @@ class Runner(object):
         # TODO(py27): Python versions < 3.5 do not support starred expressions in tuples, lists, and sets
         return (*map(sf01, (mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs)),
             mb_states, epinfos)
+
+    def process_actions(self, actions):
+        action = Action.from_gym(actions)
+        action.throttle = get_throttle(actual_speed=self.obs['speed'], target_speed=(8 * 100))
+        actions = action.as_gym()
+        return actions
+
+
 # obs, returns, masks, actions, values, neglogpacs, states = runner.run()
 
 
diff --git a/vendor/openai/baselines/ppo2/run_deepdrive.py b/vendor/openai/baselines/ppo2/run_deepdrive.py
index 2d03cbf..ab08c4f 100644
--- a/vendor/openai/baselines/ppo2/run_deepdrive.py
+++ b/vendor/openai/baselines/ppo2/run_deepdrive.py
@@ -45,7 +45,7 @@ def train(env, num_timesteps, seed, sess=None, is_discrete=True):
 
     ppo2.learn(policy=policy,
                env=env,
-               nsteps=40,
+               nsteps=80,
                nminibatches=1,  # Sweet spot is between 16 and 64 for continuous mountain car @55fps
                lam=0.95,
                gamma=0.99,