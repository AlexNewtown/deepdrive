diff --git a/agents/bootstrap_rl/train/train.py b/agents/bootstrap_rl/train/train.py
index e6767ca..4d4d7b5 100644
--- a/agents/bootstrap_rl/train/train.py
+++ b/agents/bootstrap_rl/train/train.py
@@ -18,6 +18,8 @@ class BootstrapRLGymEnv(gym.Wrapper):
         self.dagger_agent = dagger_agent
         self.previous_obz = None
 
+        self.simple_test = c.SIMPLE_PPO
+
         # One thing we need to do here is to make each action a bi-modal guassian to avoid averaging 50/50 decisions
         # i.e. half the time we veer left, half the time veer right - but on average this is go straight and can run us
         # into an obstacle. right now the DiagGaussianPd is just adding up errors which would not be the right
@@ -25,10 +27,15 @@ class BootstrapRLGymEnv(gym.Wrapper):
         # independent which is not the case (steering at higher speeds causes more acceleration a = v**2/r),
         # so that may be a problem as well.
 
+        if self.simple_test:
+            shape = (5,)
+        else:
+            shape = (dagger_agent.net.num_last_hidden + dagger_agent.net.num_targets,)
+
         self.observation_space = spaces.Box(low=np.finfo(np.float32).min,
                                             high=np.finfo(np.float32).max,
                                             # shape=(c.ALEXNET_FC7 + c.NUM_TARGETS,),
-                                            shape=(dagger_agent.net.num_last_hidden + dagger_agent.net.num_targets,),
+                                            shape=shape,
                                             dtype=np.float32)
 
     def step(self, action):
@@ -37,14 +44,16 @@ class BootstrapRLGymEnv(gym.Wrapper):
             # is not disincentivized by gforce penalty.
             action[Action.THROTTLE_INDEX] = get_throttle(actual_speed=self.previous_obz['speed'],
                                                          target_speed=(8 * 100))
-
         obz, reward, done, info = self.env.step(action)
         self.previous_obz = obz
         action, net_out = self.dagger_agent.act(obz, reward, done)
         if net_out is None:
             obz = None
         else:
-            obz = np.concatenate((np.squeeze(net_out[0]), np.squeeze(net_out[1])))
+            if self.simple_test:
+                obz = np.array([np.squeeze(a) for a in action])
+            else:
+                obz = np.concatenate((np.squeeze(net_out[0]), np.squeeze(net_out[1])))
         return obz, reward, done, info
 
     def reset(self):
@@ -88,7 +97,14 @@ def run(env_id, bootstrap_net_path,
             # Wrap step so we get the pretrained layer activations rather than pixels for our observation
             bootstrap_gym_env = BootstrapRLGymEnv(dagger_gym_env, dagger_agent)
 
-            train(bootstrap_gym_env, seed=c.RNG_SEED, sess=sess_2, is_discrete=is_discrete)
+            if c.SIMPLE_PPO:
+                mlp_width = 5
+                minibatch_steps = 16
+            else:
+                minibatch_steps = 80
+                mlp_width = 64
+            train(bootstrap_gym_env, seed=c.RNG_SEED, sess=sess_2, is_discrete=is_discrete,
+                  minibatch_steps=minibatch_steps, mlp_width=mlp_width)
     #
     # action = deepdrive.action()
     # while not done:
diff --git a/agents/dagger/agent.py b/agents/dagger/agent.py
index d2c06f9..c020899 100644
--- a/agents/dagger/agent.py
+++ b/agents/dagger/agent.py
@@ -357,7 +357,7 @@ def run(experiment, env_id='Deepdrive-v0', should_record=False, net_path=None, s
     use_sim_start_command_first_lap = c.SIM_START_COMMAND is not None
     gym_env = deepdrive.start(experiment, env_id, should_benchmark=should_benchmark, cameras=cameras,
                               use_sim_start_command=use_sim_start_command_first_lap, render=render, fps=fps,
-                              driving_style=driving_style, is_sync=is_sync)
+                              driving_style=driving_style, is_sync=is_sync, reset_returns_zero=False)
     dd_env = gym_env.env
 
     agent = Agent(gym_env.action_space, sess, env=gym_env.env,
diff --git a/config.py b/config.py
index d862a80..dde6610 100644
--- a/config.py
+++ b/config.py
@@ -121,3 +121,7 @@ DEFAULT_CAM = dict(name='forward cam 227x227 60 FOV', field_of_view=60, capture_
                    relative_rotation=[0.0, 0.0, 0.0])
 
 DEFAULT_FPS = 8
+
+
+# Experimental stuff - not worth passing as main.py args yet, but better for reproducing to put here than in os.environ
+SIMPLE_PPO = True
diff --git a/deepdrive.py b/deepdrive.py
index 574d4f4..56b0fcb 100644
--- a/deepdrive.py
+++ b/deepdrive.py
@@ -16,7 +16,7 @@ log = logs.get_log(__name__)
 def start(experiment_name=None, env='Deepdrive-v0', sess=None, start_dashboard=True, should_benchmark=True,
           cameras=None, use_sim_start_command=False, render=False, fps=c.DEFAULT_FPS, combine_box_action_spaces=False,
           is_discrete=False, preprocess_with_tensorflow=False, is_sync=False,
-          driving_style=DrivingStyle.NORMAL):
+          driving_style=DrivingStyle.NORMAL, reset_returns_zero=True):
     env = gym.make(env)
     env.seed(c.RNG_SEED)
 
@@ -29,6 +29,7 @@ def start(experiment_name=None, env='Deepdrive-v0', sess=None, start_dashboard=T
     raw_env.is_discrete = is_discrete
     raw_env.preprocess_with_tensorflow = preprocess_with_tensorflow
     raw_env.is_sync = is_sync
+    raw_env.reset_returns_zero = reset_returns_zero
     raw_env.init_action_space()
     raw_env.fps = fps
     raw_env.experiment = experiment_name.replace(' ', '_')
diff --git a/gym_deepdrive/envs/deepdrive_gym_env.py b/gym_deepdrive/envs/deepdrive_gym_env.py
index 1acdb0c..da6189e 100644
--- a/gym_deepdrive/envs/deepdrive_gym_env.py
+++ b/gym_deepdrive/envs/deepdrive_gym_env.py
@@ -221,6 +221,9 @@ class DeepDriveEnv(gym.Env):
         self.period = None  # type: float
         self.experiment = None  # type: str
         self.driving_style = None  # type: DrivingStyle
+        self.reset_returns_zero = None  # type: bool
+        self.started_driving_wrong_way_time = None  # type: bool
+        self.previous_distance_along_route = None  # type: bool
 
         if not c.REUSE_OPEN_SIM:
             if utils.get_sim_bin_path() is None:
@@ -403,9 +406,9 @@ class DeepDriveEnv(gym.Env):
             self.dashboard_pub.put(OrderedDict({'display_stats': list(self.display_stats.items()), 'should_stop': False}))
             log.debug('dashboard put took %fs', time.time() - start_dashboard_put)
 
-        if self.is_stuck(obz):  # TODO: derive this from collision, time elapsed, and distance as well
+        if self.is_stuck(obz) or self.driving_wrong_way():  # TODO: derive this from collision, time elapsed, and distance as well
             done = True
-            reward -= -10000  # reward is in scale of meters
+            reward -= 10000  # reward is in scale of meters
         info = {}
         self.step_num += 1
         log.debug('reward stuff took %fs', time.time() - start_reward_stuff)
@@ -528,6 +531,8 @@ class DeepDriveEnv(gym.Env):
         if 'distance_along_route' in obz:
             dist = obz['distance_along_route'] - self.start_distance_along_route
             progress = dist - self.distance_along_route
+            if self.distance_along_route:
+                self.previous_distance_along_route = self.distance_along_route
             self.distance_along_route = dist
             progress_reward, speed_reward = DeepDriveRewardCalculator.get_progress_reward(progress, time_passed)
 
@@ -569,7 +574,7 @@ class DeepDriveEnv(gym.Env):
             self.steps_crawling += 1
             if obz['throttle'] > 0 and obz['brake'] == 0 and obz['handbrake'] == 0:
                 self.steps_crawling_with_throttle_on += 1
-            time_crawling = time.time() - self.last_forward_progress_time
+            time_crawling = time.time() - self.last_not_stuck_time
             portion_crawling = self.steps_crawling_with_throttle_on / max(1, self.steps_crawling)
             if self.steps_crawling_with_throttle_on > 20 and time_crawling > 2 and portion_crawling > 0.8:
                 log.warn('No progress made while throttle on - assuming stuck and ending episode. steps crawling: %r, '
@@ -656,7 +661,7 @@ class DeepDriveEnv(gym.Env):
 
     # noinspection PyAttributeOutsideInit
     def set_forward_progress(self):
-        self.last_forward_progress_time = time.time()
+        self.last_not_stuck_time = time.time()
         self.steps_crawling_with_throttle_on = 0
         self.steps_crawling = 0
 
@@ -669,7 +674,11 @@ class DeepDriveEnv(gym.Env):
         self.prev_step_time = None
         self.score = Score()
         self.start_time = time.time()
+        self.started_driving_wrong_way_time = None
         log.info('Reset complete')
+        if self.reset_returns_zero:
+            # TODO: Always return zero after testing that everything works with dagger agents
+            return 0
 
     def change_viewpoint(self, cameras, use_sim_start_command):
         self.use_sim_start_command = use_sim_start_command
@@ -775,7 +784,7 @@ class DeepDriveEnv(gym.Env):
             self.change_has_control(action.has_control)
 
         if action.handbrake:
-            log.warn('Not expecting any handbraking right now! What\'s happening?! Disabling - hack :D')
+            log.debug('Not expecting any handbraking right now! What\'s happening?! Disabling - hack :D')
             action.handbrake = False
 
         action.clip()
@@ -965,6 +974,23 @@ class DeepDriveEnv(gym.Env):
         else:
             self.release_agent_control()
 
+    def driving_wrong_way(self):
+        if None in [self.previous_distance_along_route, self.distance_along_route]:
+            return False
+
+        if self.distance_along_route < self.previous_distance_along_route:
+            now = time.time()
+            s = self.started_driving_wrong_way_time
+            if s is not None:
+                if (now - s) > 2:
+                    return True
+            else:
+                self.started_driving_wrong_way_time = now
+
+        else:
+            self.started_driving_wrong_way_time = None
+        return False
+
 
 class DeepDriveRewardCalculator(object):
     @staticmethod
diff --git a/vendor/openai/baselines/a2c/utils.py b/vendor/openai/baselines/a2c/utils.py
index 0964af8..146c49a 100644
--- a/vendor/openai/baselines/a2c/utils.py
+++ b/vendor/openai/baselines/a2c/utils.py
@@ -62,7 +62,10 @@ def fc(x, scope, nh, *, init_scale=1.0, init_bias=0.0):
     with tf.variable_scope(scope):
         nin = x.get_shape()[1].value
         w = tf.get_variable("w", [nin, nh], initializer=ortho_init(init_scale))
+        w = tf.Print(w, ['w ', scope, w], summarize=100)
         b = tf.get_variable("b", [nh], initializer=tf.constant_initializer(init_bias))
+        b = tf.Print(b, ['b ', scope, w], summarize=100)
+
         return tf.matmul(x, w)+b
 
 def batch_to_seq(h, nbatch, nsteps, flat=False):
diff --git a/vendor/openai/baselines/common/vec_env/dummy_vec_env.py b/vendor/openai/baselines/common/vec_env/dummy_vec_env.py
index 2bf2177..0c5a6c7 100644
--- a/vendor/openai/baselines/common/vec_env/dummy_vec_env.py
+++ b/vendor/openai/baselines/common/vec_env/dummy_vec_env.py
@@ -26,8 +26,10 @@ class DummyVecEnv(VecEnv):
             obs_tuple, self.buf_rews[i], self.buf_dones[i], self.buf_infos[i] = self.envs[i].step(self.actions[i])
             if self.buf_dones[i]:
                 obs_tuple = self.envs[i].reset()
+            if obs_tuple is None:
+                obs_tuple = 0.
             if isinstance(obs_tuple, (tuple, list)):
-                for t,x in enumerate(obs_tuple):
+                for t, x in enumerate(obs_tuple):
                     self.buf_obs[t][i] = x
             else:
                 self.buf_obs[0][i] = obs_tuple
diff --git a/vendor/openai/baselines/ppo2/policies.py b/vendor/openai/baselines/ppo2/policies.py
index 6565d3d..7d96251 100644
--- a/vendor/openai/baselines/ppo2/policies.py
+++ b/vendor/openai/baselines/ppo2/policies.py
@@ -2,6 +2,7 @@ import numpy as np
 import tensorflow as tf
 from vendor.openai.baselines.common.distributions import make_pdtype
 from gym import spaces
+import config as c
 
 from vendor.openai.baselines.a2c.utils import conv, fc, conv_to_fc, batch_to_seq, seq_to_batch, lstm, lnlstm
 from vendor.openai.baselines.ppo2.ppo2 import TF_VAR_SCOPE
@@ -196,8 +197,8 @@ class MlpPolicy(object):
         actdim = ac_space.shape[0]
         X = tf.placeholder(tf.float32, ob_shape, name='Ob') #obs
         with tf.variable_scope(TF_VAR_SCOPE, reuse=reuse):
-            # activ = tf.tanh  # Diverges even at super low learning rates
-            activ = tf.nn.relu
+            activ = tf.tanh  # Diverges even at super low learning rates
+            # activ = tf.nn.relu
             # activ = tf.nn.leaky_relu  # Diverges
             p_h1 = activ(fc(X, 'pi_fc1', nh=self.width, init_scale=np.sqrt(2)))
             p_h2 = activ(fc(p_h1, 'pi_fc2', nh=self.width, init_scale=np.sqrt(2)))
@@ -210,6 +211,7 @@ class MlpPolicy(object):
 
         pdparam = tf.concat([pi, pi * 0.0 + logstd], axis=1)
 
+        self.p_h1 = p_h1
         self.pdtype = make_pdtype(ac_space)
         self.pd = self.pdtype.pdfromflat(pdparam)
 
@@ -225,7 +227,11 @@ class MlpPolicy(object):
         self.initial_state = None
 
         def step(ob, *_args, **_kwargs):
-            a, v, neglogp = sess.run([a0, vf, neglogp0], {X:ob})
+            if c.SIMPLE_PPO:
+                a, v, neglogp, p_w0 = sess.run([a0, vf, neglogp0, self.p_h1], {X:ob})
+                print('pw0', p_w0)
+            else:
+                a, v, neglogp = sess.run([a0, vf, neglogp0], {X: ob})
 
             # For deepdrive we expect outputs to be between -1 and 1 - let's just max out actions for now
             # a = np.tanh(a)
diff --git a/vendor/openai/baselines/ppo2/ppo2.py b/vendor/openai/baselines/ppo2/ppo2.py
index 8235e65..59beb8c 100644
--- a/vendor/openai/baselines/ppo2/ppo2.py
+++ b/vendor/openai/baselines/ppo2/ppo2.py
@@ -60,7 +60,8 @@ class Model(object):
 
         def train(lr, cliprange, obs, returns, masks, actions, values, neglogpacs, states=None):
             advs = returns - values
-            advs = (advs - advs.mean()) / (advs.std() + 1e-8)
+            if len(advs) > 1:
+                advs = (advs - advs.mean()) / (advs.std() + 1e-8)
             for _adv in advs:
                 if math.isnan(_adv):
                     print('huh oh nan time')
@@ -69,12 +70,11 @@ class Model(object):
             if states is not None:
                 td_map[train_model.S] = states
                 td_map[train_model.M] = masks
-            print('Running graph')
+            print('running backprop')
             ret = sess.run(
                 [pg_loss, vf_loss, entropy, approxkl, clipfrac, _train],
                 td_map
             )[:-1]
-            print('Done running graph')
             return ret
         self.loss_names = ['policy_loss', 'value_loss', 'policy_entropy', 'approxkl', 'clipfrac']
 
diff --git a/vendor/openai/baselines/ppo2/run_deepdrive.py b/vendor/openai/baselines/ppo2/run_deepdrive.py
index a6a17ca..2b7e00a 100644
--- a/vendor/openai/baselines/ppo2/run_deepdrive.py
+++ b/vendor/openai/baselines/ppo2/run_deepdrive.py
@@ -2,12 +2,14 @@
 
 import os
 
+import config as c
+
 from vendor.openai.baselines import bench, logger
 
 from vendor.openai.baselines.common.cmd_util import continuous_mountain_car_arg_parser
 
 
-def train(env, seed, sess=None, is_discrete=True, minibatch_steps=80, mlp_width=64):
+def train(env, seed, sess=None, is_discrete=True, minibatch_steps=None, mlp_width=None):
     from vendor.openai.baselines.common.misc_util import set_global_seeds
     from vendor.openai.baselines.common.vec_env.vec_normalize import VecNormalize
     from vendor.openai.baselines.ppo2 import ppo2
@@ -26,8 +28,11 @@ def train(env, seed, sess=None, is_discrete=True, minibatch_steps=80, mlp_width=
         tf.Session(config=config).__enter__()
 
     env = DummyVecEnv(envs=[env])
-    # env = VecNormalize(env, ob=False)
-    env = VecNormalize(env)
+
+    if c.SIMPLE_PPO:
+        env = VecNormalize(env, ob=False)
+    else:
+        env = VecNormalize(env)
 
     set_global_seeds(seed)
     if is_discrete:
@@ -51,7 +56,7 @@ def train(env, seed, sess=None, is_discrete=True, minibatch_steps=80, mlp_width=
                noptepochs=3,
                log_interval=1,
                ent_coef=0.0,
-               lr=lambda f: f * 2.5e-2,
+               lr=lambda f: f * 2.5e-3,
                cliprange=lambda f: f * 0.1,
                total_timesteps=int(1e5),
                mlp_width=mlp_width)