diff --git a/agents/bootstrap_rl/train/train.py b/agents/bootstrap_rl/train/train.py
index 3f1c468..79d02d5 100644
--- a/agents/bootstrap_rl/train/train.py
+++ b/agents/bootstrap_rl/train/train.py
@@ -34,9 +34,9 @@ class BootstrapRLGymEnv(gym.Wrapper):
         if net_out is None:
             obz = None
         else:
-            est_min_action = -1.5e-8
-            est_max_action = 1.5e-8
-            actions = np.squeeze(net_out[0]) / (est_max_action - est_min_action) * 2  # scale to -1 - 1
+            # est_min_action = -1.5e-8
+            # est_max_action = 1.5e-8
+            # actions = np.squeeze(net_out[0]) / (est_max_action - est_min_action) * 2  # scale to -1 - 1
 
             obz = np.concatenate((np.squeeze(net_out[0]), np.squeeze(net_out[1])))
         return obz, reward, done, info
diff --git a/vendor/openai/baselines/ppo2/run_deepdrive.py b/vendor/openai/baselines/ppo2/run_deepdrive.py
index 9022767..553eb48 100644
--- a/vendor/openai/baselines/ppo2/run_deepdrive.py
+++ b/vendor/openai/baselines/ppo2/run_deepdrive.py
@@ -47,7 +47,7 @@ def train(env, num_timesteps, seed, sess=None, is_discrete=True):
                noptepochs=3,
                log_interval=1,
                ent_coef=0.0,
-               lr=lambda f: f * 2.5e-8,
+               lr=lambda f: f * 0.,
                cliprange=lambda f: f * 0.1,
                total_timesteps=num_timesteps)