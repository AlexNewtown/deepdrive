diff --git a/agents/bootstrap_rl/train/train.py b/agents/bootstrap_rl/train/train.py
index 854c5fc..3f1c468 100644
--- a/agents/bootstrap_rl/train/train.py
+++ b/agents/bootstrap_rl/train/train.py
@@ -53,26 +53,35 @@ def run(env_id, bootstrap_net_path,
         intra_op_parallelism_threads=1,
         inter_op_parallelism_threads=1,
         gpu_options=tf.GPUOptions(
-            per_process_gpu_memory_fraction=0.8,
+            per_process_gpu_memory_fraction=0.4,
             # leave room for the game,
             # NOTE: debugging python, i.e. with PyCharm can cause OOM errors, where running will not
             allow_growth=True
         ),
     )
 
-    sess = tf.Session(config=tf_config)
-    with sess.as_default():
-        dagger_gym_env = deepdrive.start(experiment, env_id, cameras=camera_rigs, render=render, fps=fps,
-                                         combine_box_action_spaces=True, is_sync=is_sync)
+    g_1 = tf.Graph()
+    with g_1.as_default():
+        sess_1 = tf.Session(config=tf_config)
 
-        dagger_agent = Agent(dagger_gym_env.action_space, sess, env=dagger_gym_env.env,
-                             should_record_recovery_from_random_actions=False, should_record=should_record,
-                             net_path=bootstrap_net_path, output_last_hidden=True, net_name=MOBILENET_V2_NAME)
+        with sess_1.as_default():
+            dagger_gym_env = deepdrive.start(experiment, env_id, cameras=camera_rigs, render=render, fps=fps,
+                                             combine_box_action_spaces=True, is_sync=is_sync)
 
-        # Wrap step so we get the pretrained layer activations rather than pixels for our observation
-        bootstrap_gym_env = BootstrapRLGymEnv(dagger_gym_env, dagger_agent)
+            dagger_agent = Agent(dagger_gym_env.action_space, sess_1, env=dagger_gym_env.env,
+                                 should_record_recovery_from_random_actions=False, should_record=should_record,
+                                 net_path=bootstrap_net_path, output_last_hidden=True, net_name=MOBILENET_V2_NAME)
 
-        train(bootstrap_gym_env, num_timesteps=int(10e6), seed=c.RNG_SEED, sess=sess, is_discrete=is_discrete)
+    g_2 = tf.Graph()
+    with g_2.as_default():
+        sess_2 = tf.Session(config=tf_config)
+
+        with sess_2.as_default():
+
+            # Wrap step so we get the pretrained layer activations rather than pixels for our observation
+            bootstrap_gym_env = BootstrapRLGymEnv(dagger_gym_env, dagger_agent)
+
+            train(bootstrap_gym_env, num_timesteps=int(10e6), seed=c.RNG_SEED, sess=sess_2, is_discrete=is_discrete)
     #
     # action = deepdrive.action()
     # while not done:
diff --git a/agents/dagger/agent.py b/agents/dagger/agent.py
index a954d14..4731d45 100644
--- a/agents/dagger/agent.py
+++ b/agents/dagger/agent.py
@@ -325,9 +325,17 @@ def run(experiment, env_id='Deepdrive-v0', should_record=False, net_path=None, s
     reward = 0
     episode_done = False
     max_episodes = 1000
+
+    # The following will work with 4GB vram
+    if net_name == net.ALEXNET_NAME:
+        per_process_gpu_memory_fraction = 0.8
+    else:
+        per_process_gpu_memory_fraction = 0.4
+
+
     tf_config = tf.ConfigProto(
         gpu_options=tf.GPUOptions(
-            per_process_gpu_memory_fraction=0.8,
+            per_process_gpu_memory_fraction=per_process_gpu_memory_fraction,
             # leave room for the game,
             # NOTE: debugging python, i.e. with PyCharm can cause OOM errors, where running will not
             allow_growth=True
diff --git a/agents/dagger/train/train.py b/agents/dagger/train/train.py
index d2154f3..9597c01 100644
--- a/agents/dagger/train/train.py
+++ b/agents/dagger/train/train.py
@@ -43,6 +43,9 @@ def visualize_gradients(grads_and_vars):
 
 def run(resume_dir=None, data_dir=c.RECORDING_DIR, agent_name=None, overfit=False, eval_only=False, tf_debug=False,
         freeze_pretrained=False):
+
+    # TODO: Don't use generic word like 'model' here that other projects often use.
+    # Need to rename/retrain saved models tho...
     with tf.variable_scope("model"):
         global_step = tf.get_variable("global_step", [], tf.int32, initializer=tf.zeros_initializer, trainable=False)
     agent_net = get_agent_net(agent_name, global_step, eval_only=eval_only, freeze_pretrained=freeze_pretrained)
diff --git a/gym_deepdrive/envs/deepdrive_gym_env.py b/gym_deepdrive/envs/deepdrive_gym_env.py
index e24d0c7..2ac6f2a 100644
--- a/gym_deepdrive/envs/deepdrive_gym_env.py
+++ b/gym_deepdrive/envs/deepdrive_gym_env.py
@@ -1,6 +1,8 @@
 from __future__ import (absolute_import, division,
                         print_function, unicode_literals)
 
+import math
+
 from future.builtins import (ascii, bytes, chr, dict, filter, hex, input,
                              int, map, next, oct, open, pow, range, round,
                              str, super, zip)
@@ -81,7 +83,7 @@ class Action(object):
             else:
                 has_control = action[4]
         handbrake = action[3][0]
-        if handbrake <= 0:
+        if handbrake <= 0 or math.isnan(handbrake):
             handbrake = 0
         else:
             handbrake = 1
diff --git a/vendor/openai/baselines/ppo2/policies.py b/vendor/openai/baselines/ppo2/policies.py
index ee59f68..fa8d61c 100644
--- a/vendor/openai/baselines/ppo2/policies.py
+++ b/vendor/openai/baselines/ppo2/policies.py
@@ -4,6 +4,7 @@ from vendor.openai.baselines.common.distributions import make_pdtype
 from gym import spaces
 
 from vendor.openai.baselines.a2c.utils import conv, fc, conv_to_fc, batch_to_seq, seq_to_batch, lstm, lnlstm
+from vendor.openai.baselines.ppo2.ppo2 import TF_VAR_SCOPE
 
 
 def nature_cnn(unscaled_images):
@@ -27,7 +28,7 @@ class LnLstmPolicy(object):
         X = tf.placeholder(tf.uint8, ob_shape) #obs
         M = tf.placeholder(tf.float32, [nbatch]) #mask (done t-1)
         S = tf.placeholder(tf.float32, [nenv, nlstm*2]) #states
-        with tf.variable_scope("model", reuse=reuse):
+        with tf.variable_scope(TF_VAR_SCOPE, reuse=reuse):
             h = nature_cnn(X)
             xs = batch_to_seq(h, nenv, nsteps)
             ms = batch_to_seq(M, nenv, nsteps)
@@ -69,7 +70,7 @@ class LstmPolicy(object):
         X = tf.placeholder(tf.uint8, ob_shape) #obs
         M = tf.placeholder(tf.float32, [nbatch]) #mask (done t-1)
         S = tf.placeholder(tf.float32, [nenv, nlstm*2]) #states
-        with tf.variable_scope("model", reuse=reuse):
+        with tf.variable_scope(TF_VAR_SCOPE, reuse=reuse):
             h = nature_cnn(X)
             xs = batch_to_seq(h, nenv, nsteps)
             ms = batch_to_seq(M, nenv, nsteps)
@@ -113,7 +114,7 @@ class LstmPolicyFlat(object):
         X = tf.placeholder(tf.uint8, ob_shape) #obs
         M = tf.placeholder(tf.float32, [nbatch]) #mask (done t-1)
         S = tf.placeholder(tf.float32, [nenv, nlstm*2]) #states
-        with tf.variable_scope("model", reuse=reuse):
+        with tf.variable_scope(TF_VAR_SCOPE, reuse=reuse):
             h = tf.cast(X, tf.float32)
             xs = batch_to_seq(h, nenv, nsteps)
             ms = batch_to_seq(M, nenv, nsteps)
@@ -161,7 +162,7 @@ class CnnPolicy(object):
         ob_shape = (nbatch, nh, nw, nc)
         nact = ac_space.n
         X = tf.placeholder(tf.uint8, ob_shape) #obs
-        with tf.variable_scope("model", reuse=reuse):
+        with tf.variable_scope(TF_VAR_SCOPE, reuse=reuse):
             h = nature_cnn(X)
             pi = fc(h, 'pi', nact, init_scale=0.01)
             vf = fc(h, 'v', 1)[:,0]
@@ -192,7 +193,7 @@ class MlpPolicy(object):
         ob_shape = (nbatch,) + ob_space.shape
         actdim = ac_space.shape[0]
         X = tf.placeholder(tf.float32, ob_shape, name='Ob') #obs
-        with tf.variable_scope("model", reuse=reuse):
+        with tf.variable_scope(TF_VAR_SCOPE, reuse=reuse):
             activ = tf.tanh
             h1 = activ(fc(X, 'pi_fc1', nh=64, init_scale=np.sqrt(2)))
             h2 = activ(fc(h1, 'pi_fc2', nh=64, init_scale=np.sqrt(2)))
diff --git a/vendor/openai/baselines/ppo2/ppo2.py b/vendor/openai/baselines/ppo2/ppo2.py
index 0814250..b04b64f 100644
--- a/vendor/openai/baselines/ppo2/ppo2.py
+++ b/vendor/openai/baselines/ppo2/ppo2.py
@@ -11,6 +11,8 @@ from vendor.openai.baselines import logger
 
 from vendor.openai.baselines.common.math_util import explained_variance
 
+TF_VAR_SCOPE = 'ppo2model'
+
 
 class Model(object):
     def __init__(self, *, policy, ob_space, ac_space, nbatch_act, nbatch_train,
@@ -43,7 +45,7 @@ class Model(object):
         approxkl = .5 * tf.reduce_mean(tf.square(neglogpac - OLDNEGLOGPAC))
         clipfrac = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), CLIPRANGE)))
         loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef
-        with tf.variable_scope('model'):
+        with tf.variable_scope(TF_VAR_SCOPE):
             params = tf.trainable_variables()
         grads = tf.gradients(loss, params)
         if max_grad_norm is not None: