diff --git a/agents/dagger/agent.py b/agents/dagger/agent.py
index b0fb9c1..13e7100 100644
--- a/agents/dagger/agent.py
+++ b/agents/dagger/agent.py
@@ -12,7 +12,7 @@ import config as c
 import deepdrive
 from agents.dagger import net
 from agents.dagger.train.train import resize_images
-from gym_deepdrive.envs.deepdrive_gym_env import Action
+from gym_deepdrive.envs.deepdrive_gym_env import Action, Urgency
 from agents.dagger.net import AlexNet, MobileNetV2
 from utils import save_hdf5, download
 import logs
@@ -24,7 +24,7 @@ class Agent(object):
     def __init__(self, action_space, tf_session, env, should_record_recovery_from_random_actions=True,
                  should_record=False, net_path=None, use_frozen_net=False, random_action_count=0,
                  non_random_action_count=5, path_follower=False, recording_dir=c.RECORDING_DIR, output_last_hidden=False,
-                 net_name=net.ALEXNET_NAME):
+                 net_name=net.ALEXNET_NAME, urgency=Urgency.NORMAL):
         np.random.seed(c.RNG_SEED)
         self.action_space = action_space
         self.previous_action = None
@@ -32,6 +32,7 @@ class Agent(object):
         self.step = 0
         self.env = env
         self.net_name = net_name
+        self.urgency = urgency
 
         # State for toggling random actions
         self.should_record_recovery_from_random_actions = should_record_recovery_from_random_actions
@@ -154,7 +155,16 @@ class Agent(object):
 
         else:
             # AlexNet
-            target_speed = 9 * 100
+
+            if self.urgency == Urgency.CRUISING:
+                target_speed = 8 * 100
+            elif self.urgency == Urgency.NORMAL:
+                target_speed = 9 * 100
+            elif self.urgency == Urgency.LATE:
+                target_speed = 10 * 100
+            else:
+                raise NotImplementedError('Urgency level not supported')
+
             # Network overfit on speed, plus it's nice to be able to change it,
             # so we just ignore output speed of net
             desired_throttle = abs(target_speed / max(actual_speed, 1e-3))
@@ -305,10 +315,10 @@ class Agent(object):
             self.semirandom_sequence_step += 1
 
 
-def run(experiment, env_id='DeepDrivePreproTensorflow-v0', should_record=False, net_path=None, should_benchmark=True,
+def run(experiment, env_id='Deepdrive-v0', should_record=False, net_path=None, should_benchmark=True,
         run_baseline_agent=False, camera_rigs=None, should_rotate_sim_types=False,
         should_record_recovery_from_random_actions=False, render=False, path_follower=False, fps=c.DEFAULT_FPS,
-        net_name=net.ALEXNET_NAME):
+        net_name=net.ALEXNET_NAME, urgency=Urgency.NORMAL):
     if run_baseline_agent:
         net_path = ensure_baseline_weights(net_path)
     reward = 0
@@ -338,14 +348,14 @@ def run(experiment, env_id='DeepDrivePreproTensorflow-v0', should_record=False,
 
     use_sim_start_command_first_lap = c.SIM_START_COMMAND is not None
     gym_env = deepdrive.start(experiment, env_id, should_benchmark=should_benchmark, cameras=cameras,
-                                  use_sim_start_command=use_sim_start_command_first_lap, render=render,
-                                  fps=fps)
+                              use_sim_start_command=use_sim_start_command_first_lap, render=render, fps=fps,
+                              urgency=urgency)
     dd_env = gym_env.env
 
     agent = Agent(gym_env.action_space, sess, env=gym_env.env,
                   should_record_recovery_from_random_actions=should_record_recovery_from_random_actions,
                   should_record=should_record, net_path=net_path, random_action_count=4, non_random_action_count=5,
-                  path_follower=path_follower, net_name=net_name)
+                  path_follower=path_follower, net_name=net_name, urgency=urgency)
     if net_path:
         log.info('Running tensorflow agent checkpoint: %s', net_path)
 
diff --git a/deepdrive.py b/deepdrive.py
index 8000efe..d11d5df 100644
--- a/deepdrive.py
+++ b/deepdrive.py
@@ -7,16 +7,16 @@ import config as c
 import random_name
 
 # noinspection PyUnresolvedReferences
-from gym_deepdrive.envs.deepdrive_gym_env import gym_action as action, RushLevel
+from gym_deepdrive.envs.deepdrive_gym_env import gym_action as action, Urgency
 from vendor.openai.baselines.common.continuous_action_wrapper import CombineBoxSpaceWrapper
 
 log = logs.get_log(__name__)
 
 
-def start(experiment_name=None, env='DeepDrive-v0', sess=None, start_dashboard=True, should_benchmark=True,
+def start(experiment_name=None, env='Deepdrive-v0', sess=None, start_dashboard=True, should_benchmark=True,
           cameras=None, use_sim_start_command=False, render=False, fps=c.DEFAULT_FPS, combine_box_action_spaces=False,
           is_discrete=False, preprocess_with_tensorflow=False, is_sync=False, sync_step_time=0.125,
-          rush_level=RushLevel.NORMAL):
+          urgency=Urgency.NORMAL):
     env = gym.make(env)
     env.seed(c.RNG_SEED)
 
@@ -34,7 +34,7 @@ def start(experiment_name=None, env='DeepDrive-v0', sess=None, start_dashboard=T
     dd_env.fps = fps
     dd_env.experiment = experiment_name.replace(' ', '_')
     dd_env.period = 1. / fps
-    dd_env.rush_level = rush_level
+    dd_env.urgency = urgency
     dd_env.set_use_sim_start_command(use_sim_start_command)
     dd_env.open_sim()
     if use_sim_start_command:
diff --git a/gym_deepdrive/__init__.py b/gym_deepdrive/__init__.py
index 54d8c33..495d15b 100644
--- a/gym_deepdrive/__init__.py
+++ b/gym_deepdrive/__init__.py
@@ -3,7 +3,7 @@ from gym.envs.registration import register
 
 # Use deepdrive.start to parameterize environment. Parameterizing here leads to combinitorial splosion.
 register(
-    id='DeepDrive-v0',
+    id='Deepdrive-v0',
     entry_point='gym_deepdrive.envs.deepdrive_gym_env:DeepDriveEnv',
     kwargs=dict(),
 )
diff --git a/gym_deepdrive/envs/deepdrive_gym_env.py b/gym_deepdrive/envs/deepdrive_gym_env.py
index 3cb8f62..4c81a56 100644
--- a/gym_deepdrive/envs/deepdrive_gym_env.py
+++ b/gym_deepdrive/envs/deepdrive_gym_env.py
@@ -105,13 +105,15 @@ class DiscreteActions(object):
 
 class RewardWeighting(object):
     def __init__(self, progress, gforce, lane_deviation, total_time, rate_of_progress):
+        # Progress and time were used in DeepDrive-v0 (2.0) - keeping for now in case we want to use again
         self.progress_weight = progress
         self.gforce_weight = gforce
         self.lane_deviation_weight = lane_deviation
         self.time_weight = total_time
         self.rate_of_progress_weight = rate_of_progress
 
-    def combine(self, progress_reward, gforce_penalty, lane_deviation_penalty, time_penalty, rate_of_progress):
+    @staticmethod
+    def combine(progress_reward, gforce_penalty, lane_deviation_penalty, time_penalty, rate_of_progress):
         return progress_reward  \
                - gforce_penalty \
                - lane_deviation_penalty \
@@ -119,7 +121,7 @@ class RewardWeighting(object):
                + rate_of_progress
 
 
-class RushLevel(Enum):
+class Urgency(Enum):
     __order__ = 'CRUISING NORMAL LATE EMERGENCY CHASE'
     # TODO: Possibly assign function rather than just weights
     CRUISING  = RewardWeighting(progress=0.0, gforce=2.00, lane_deviation=1.50, total_time=0.0, rate_of_progress=0.5)
@@ -184,7 +186,7 @@ class DeepDriveEnv(gym.Env):
         self.fps = None
         self.period = None
         self.experiment = None
-        self.rush_level = None  # type: RushLevel
+        self.urgency = None  # type: Urgency
 
         if not c.REUSE_OPEN_SIM:
             if utils.get_sim_bin_path() is None:
@@ -462,7 +464,7 @@ class DeepDriveEnv(gym.Env):
             lane_deviation_penalty = DeepDriveRewardCalculator.get_lane_deviation_penalty(
                 obz['distance_to_center_of_lane'], time_passed)
 
-        lane_deviation_penalty *= self.rush_level.value.lane_deviation_weight
+        lane_deviation_penalty *= self.urgency.value.lane_deviation_weight
         self.score.lane_deviation_penalty += lane_deviation_penalty
 
         self.display_stats['lane deviation penalty']['value'] = -lane_deviation_penalty
@@ -479,7 +481,7 @@ class DeepDriveEnv(gym.Env):
                 self.display_stats['g-forces']['total'] = gforces
                 gforce_penalty = DeepDriveRewardCalculator.get_gforce_penalty(gforces, time_passed)
 
-        gforce_penalty *= self.rush_level.value.gforce_weight
+        gforce_penalty *= self.urgency.value.gforce_weight
         self.score.gforce_penalty += gforce_penalty
 
         self.display_stats['gforce penalty']['value'] = -self.score.gforce_penalty
@@ -494,8 +496,8 @@ class DeepDriveEnv(gym.Env):
             self.distance_along_route = dist
             progress_reward, rate_reward = DeepDriveRewardCalculator.get_progress_reward(progress, time_passed)
 
-        progress_reward *= self.rush_level.value.progress_weight
-        rate_reward *= self.rush_level.value.rate_of_progress_weight
+        progress_reward *= self.urgency.value.progress_weight
+        rate_reward *= self.urgency.value.rate_of_progress_weight
 
         self.score.progress_reward += progress_reward
         self.score.rate_of_progress_reward += rate_reward
@@ -511,12 +513,12 @@ class DeepDriveEnv(gym.Env):
 
     def get_time_penalty(self, _obz, time_passed):
         time_penalty = time_passed or 0
-        time_penalty *= self.rush_level.value.time_weight
+        time_penalty *= self.urgency.value.time_weight
         self.score.time_penalty += time_penalty
         return time_penalty
 
     def combine_rewards(self, progress_reward, gforce_penalty, lane_deviation_penalty, time_penalty, rate_of_progress):
-        return self.rush_level.value.combine(progress_reward, gforce_penalty, lane_deviation_penalty, time_penalty, rate_of_progress)
+        return self.urgency.value.combine(progress_reward, gforce_penalty, lane_deviation_penalty, time_penalty, rate_of_progress)
 
     def is_stuck(self, obz):
         start_is_stuck = time.time()
@@ -587,7 +589,7 @@ class DeepDriveEnv(gym.Env):
             writer.writerow(['high score', high])
             writer.writerow(['low score', low])
             writer.writerow(['env', self.spec.id])
-            writer.writerow(['args', sys.argv[1:]])
+            writer.writerow(['args', ', '.join(sys.argv[1:])])
             writer.writerow(['git commit', '@' + self.git_commit])
             writer.writerow(['experiment name', self.experiment or 'n/a'])
             writer.writerow(['os', sys.platform])
@@ -967,7 +969,7 @@ class DeepDriveRewardCalculator(object):
         progress_balance_coeff = 1.0
         progress_reward = progress * progress_balance_coeff
 
-        rate_balance_coeff = 0.05
+        rate_balance_coeff = 0.15
         rate_reward *= rate_balance_coeff
 
         progress_reward = DeepDriveRewardCalculator.clip(progress_reward)
diff --git a/main.py b/main.py
index d3e77be..d143532 100644
--- a/main.py
+++ b/main.py
@@ -11,11 +11,12 @@ import deepdrive
 import logs
 from agents.dagger import net
 from agents.dagger.agent import ensure_baseline_weights
+from gym_deepdrive.envs.deepdrive_gym_env import Urgency
 
 
 def main():
     parser = argparse.ArgumentParser(description=None)
-    parser.add_argument('-e', '--env-id', nargs='?', default='DeepDrive-v0', help='Select the environment to run')
+    parser.add_argument('-e', '--env-id', nargs='?', default='Deepdrive-v0', help='Select the environment to run')
     parser.add_argument('-r', '--record', action='store_true', default=False,
                         help='Records game driving, including recovering from random actions')
     parser.add_argument('--baseline', action='store_true', default=False,
@@ -44,6 +45,9 @@ def main():
                              'i.e. /home/a/DeepDrive/tensorflow/2018-01-01__11-11-11AM_train/model.ckpt-98331')
     parser.add_argument('--net-type', nargs='?', default=net.ALEXNET_NAME,
                         help='Your model type - i.e. AlexNet or MobileNetV2')
+    parser.add_argument('--urgency', nargs='?', default=Urgency.NORMAL.name.lower(),
+                        help='Speed vs comfort prioritization, i.e. ' +
+                             ', '.join([level.name.lower() for level in Urgency]))
     parser.add_argument('--resume-train', nargs='?', default=None,
                         help='Name of the tensorflow training session you want to resume within %s, '
                              'i.e. 2018-01-01__11-11-11AM_train' % c.TENSORFLOW_OUT_DIR)
@@ -104,7 +108,8 @@ def main():
         episode_count = 1
         gym_env = None
         try:
-            gym_env = deepdrive.start(args.experiment_name, args.env_id, fps=args.fps)
+            gym_env = deepdrive.start(args.experiment_name, args.env_id, fps=args.fps,
+                                      urgency=Urgency[args.urgency.upper()])
             log.info('Path follower drive mode')
             for episode in range(episode_count):
                 if done:
@@ -133,7 +138,8 @@ def main():
                   should_record=args.record, net_path=args.net_path, env_id=args.env_id,
                   run_baseline_agent=args.baseline, render=args.render, camera_rigs=camera_rigs,
                   should_record_recovery_from_random_actions=args.record_recovery_from_random_actions,
-                  path_follower=args.path_follower, fps=args.fps, net_name=args.net_type)
+                  path_follower=args.path_follower, fps=args.fps, net_name=args.net_type,
+                  urgency=Urgency[args.urgency.upper()])
 
 
 def get_latest_model():
diff --git a/tests/test_sanity.py b/tests/test_sanity.py
index 00b1509..3ff481c 100644
--- a/tests/test_sanity.py
+++ b/tests/test_sanity.py
@@ -67,17 +67,17 @@ def test_gforce_penalty():
 
 def test_progress_reward():
     progress_reward, rate_reward = DeepDriveRewardCalculator.get_progress_reward(progress=100, time_passed=0.1)
-    assert progress_reward == pytest.approx(1.) and rate_reward == pytest.approx(0.5)
+    assert progress_reward == pytest.approx(1.) and rate_reward == pytest.approx(1.5)
     progress_reward, rate_reward = DeepDriveRewardCalculator.get_progress_reward(progress=100, time_passed=1)
-    assert progress_reward == pytest.approx(1.) and rate_reward == pytest.approx(0.05)
+    assert progress_reward == pytest.approx(1.) and rate_reward == pytest.approx(0.15)
     progress_reward, rate_reward = DeepDriveRewardCalculator.get_progress_reward(progress=3, time_passed=0.1)
-    assert progress_reward == pytest.approx(0.03) and rate_reward == pytest.approx(0.00045)
+    assert progress_reward == pytest.approx(0.03) and rate_reward == pytest.approx(0.00135)
     progress_reward, rate_reward = DeepDriveRewardCalculator.get_progress_reward(progress=3, time_passed=1e-8)
     assert progress_reward == pytest.approx(0.03, abs=1e-6) and rate_reward == pytest.approx(100.0)  # Should clip
     progress_reward, rate_reward = DeepDriveRewardCalculator.get_progress_reward(progress=0, time_passed=0.1)
     assert progress_reward == pytest.approx(0.) and rate_reward == pytest.approx(0.)
     progress_reward, rate_reward = DeepDriveRewardCalculator.get_progress_reward(progress=-10, time_passed=0.1)
-    assert progress_reward == pytest.approx(-0.1) and rate_reward == pytest.approx(-0.005)
+    assert progress_reward == pytest.approx(-0.1) and rate_reward == pytest.approx(-0.015)
     progress_reward, rate_reward = DeepDriveRewardCalculator.get_progress_reward(progress=1e8, time_passed=0.1)
     assert progress_reward == pytest.approx(100.) and rate_reward == pytest.approx(100.)  # Should clip
     progress_reward, rate_reward = DeepDriveRewardCalculator.get_progress_reward(progress=-1e8, time_passed=0.1)