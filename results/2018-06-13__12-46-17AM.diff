diff --git a/agents/bootstrap_rl/train/train.py b/agents/bootstrap_rl/train/train.py
index a70a9be..4d4d7b5 100644
--- a/agents/bootstrap_rl/train/train.py
+++ b/agents/bootstrap_rl/train/train.py
@@ -101,7 +101,7 @@ def run(env_id, bootstrap_net_path,
                 mlp_width = 5
                 minibatch_steps = 16
             else:
-                minibatch_steps = 160
+                minibatch_steps = 80
                 mlp_width = 64
             train(bootstrap_gym_env, seed=c.RNG_SEED, sess=sess_2, is_discrete=is_discrete,
                   minibatch_steps=minibatch_steps, mlp_width=mlp_width)
diff --git a/gym_deepdrive/envs/deepdrive_gym_env.py b/gym_deepdrive/envs/deepdrive_gym_env.py
index 2ad6e9c..90e35c3 100644
--- a/gym_deepdrive/envs/deepdrive_gym_env.py
+++ b/gym_deepdrive/envs/deepdrive_gym_env.py
@@ -402,7 +402,6 @@ class DeepDriveEnv(gym.Env):
         if obz and 'is_game_driving' in obz:
             self.has_control = not obz['is_game_driving']
         now = time.time()
-        done = False
 
         start_reward_stuff = time.time()
         reward, done = self.get_reward(obz, now)
diff --git a/vendor/openai/baselines/ppo2/policies.py b/vendor/openai/baselines/ppo2/policies.py
index 7cb4b69..b8cfdef 100644
--- a/vendor/openai/baselines/ppo2/policies.py
+++ b/vendor/openai/baselines/ppo2/policies.py
@@ -232,7 +232,7 @@ class MlpPolicy(object):
             a, v, neglogp, action_probs = sess.run([a0, vf, neglogp0, action_probs0], {X: ob})
 
             # For deepdrive we expect outputs to be between -1 and 1 - let's just max out actions for now
-            # a = np.tanh(a)
+            a = np.tanh(a)
 
             return a, v, self.initial_state, neglogp, action_probs